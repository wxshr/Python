
# Python urllib æ¨¡å—å­¦ä¹ ç¬”è®°

---

## ä¸€ã€ç½‘é¡µçˆ¬å–æµç¨‹æ¦‚è§ˆ

### 1. æ˜ç¡®çˆ¬å–ç›®æ ‡

- **ç¡®å®šéœ€æ±‚**ï¼šæ˜ç¡®è¦çˆ¬å–çš„æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘ã€å•†å“ä¿¡æ¯ç­‰ï¼‰ã€‚
- **é€‰æ‹©ç›®æ ‡ç½‘ç«™**ï¼šåˆ†æç½‘ç«™ç»“æ„ï¼ˆURLè§„å¾‹ã€é¡µé¢æ¸²æŸ“æ–¹å¼ç­‰ï¼‰ã€‚
- **æ£€æŸ¥åˆæ³•æ€§**ï¼šæŸ¥çœ‹ç½‘ç«™çš„ `robots.txt` æ–‡ä»¶ï¼Œç¡®è®¤æ˜¯å¦å…è®¸çˆ¬å–ã€‚

### 2. å‘é€ HTTP è¯·æ±‚

- **å·¥å…·é€‰æ‹©**ï¼š
  - å¸¸ç”¨åº“ï¼š`urllib`ï¼ˆæ ‡å‡†åº“ï¼‰ã€`requests`ï¼ˆæ›´ç®€æ´ï¼‰ã€`aiohttp`ï¼ˆå¼‚æ­¥ï¼‰ã€‚
  - æ¨¡æ‹Ÿæµè§ˆå™¨ï¼š`selenium`ã€`playwright`ï¼ˆå¤„ç†åŠ¨æ€é¡µé¢ï¼‰ã€‚

- **è¯·æ±‚ç±»å‹**ï¼š
  - `GET`ï¼šç”¨äºè·å–æ•°æ®ã€‚
  - `POST`ï¼šç”¨äºæäº¤è¡¨å•ç­‰ã€‚

- **è®¾ç½®è¯·æ±‚å¤´ï¼ˆå¦‚ User-Agentï¼‰**ï¼šæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«åçˆ¬ã€‚

### 3. è§£æç½‘é¡µå†…å®¹

- **é™æ€é¡µé¢ï¼ˆHTMLï¼‰**ï¼š
  - å·¥å…·ï¼š`BeautifulSoup`ã€`lxml`ã€æ­£åˆ™è¡¨è¾¾å¼ç­‰ã€‚
  
- **åŠ¨æ€é¡µé¢ï¼ˆAJAX/JS æ¸²æŸ“ï¼‰**ï¼š
  - ä½¿ç”¨ `selenium`/`playwright` æ¨¡æ‹Ÿæµè§ˆå™¨ã€‚
  - æˆ–é€šè¿‡æµè§ˆå™¨å¼€å‘è€…å·¥å…·æŠ“æ¥å£ã€‚

### 4. æ•°æ®æå–ä¸æ¸…æ´—

- **å­—æ®µæå–**ï¼šå¦‚æ ‡é¢˜ã€ä»·æ ¼ã€è¯„è®ºç­‰ã€‚
- **æ¸…æ´—æŠ€å·§**ï¼š
  - å»é™¤ç©ºç™½å­—ç¬¦ï¼š`strip()`ã€‚
  - ç¼–ç å¤„ç†ï¼šå¦‚ `decode('utf-8')`ã€‚
  - è¿‡æ»¤ç©ºå€¼ã€å¹¿å‘Šç­‰æ— æ•ˆä¿¡æ¯ã€‚

### 5. æ•°æ®å­˜å‚¨

- **æ–‡ä»¶å­˜å‚¨**ï¼šCSVï¼ˆ`pandas`ï¼‰ã€JSONã€æ–‡æœ¬ç­‰ã€‚
- **æ•°æ®åº“**ï¼šMySQLï¼ˆ`pymysql`ï¼‰ã€MongoDBï¼ˆ`pymongo`ï¼‰ç­‰ã€‚

### 6. åçˆ¬æœºåˆ¶åº”å¯¹

- éšæœº UAã€IPä»£ç†ã€éªŒè¯ç è¯†åˆ«ã€é™é€Ÿçˆ¬å–ï¼ˆ`time.sleep()`ï¼‰ã€‚
- åˆç†è®¾ç½®è¯·æ±‚é¢‘ç‡ï¼Œé¿å…å° IPã€‚

---

## äºŒã€urllib æ¨¡å—è¯¦è§£

`urllib` æ˜¯ Python æ ‡å‡†åº“ä¸­å¤„ç† URL çš„æ¨¡å—ï¼Œæ”¯æŒè¯·æ±‚å‘é€ã€ç¼–ç è§£æã€å¼‚å¸¸å¤„ç†ç­‰åŠŸèƒ½ã€‚

### 2.1 æ¨¡å—ç»“æ„

| å­æ¨¡å— | åŠŸèƒ½ |
|--------|------|
| `urllib.request` | å‘é€è¯·æ±‚ |
| `urllib.parse` | URL è§£æä¸æ„å»º |
| `urllib.error` | è¯·æ±‚å¼‚å¸¸å¤„ç† |
| `urllib.robotparser` | è§£æ `robots.txt` |

---

## ä¸‰ã€urllib.request ä½¿ç”¨è¯¦è§£

### 3.1 å‘é€ GET è¯·æ±‚

```python
from urllib.request import urlopen

url = "http://www.baidu.com"
response = urlopen(url)

# è¯»å–å“åº”å†…å®¹
content = response.read().decode('utf-8')
print(content)
```

### 3.2 å“åº”å¯¹è±¡å¸¸ç”¨æ–¹æ³•

```python
from urllib.request import urlopen
import http.client

response: http.client.HTTPResponse = urlopen("http://www.baidu.com")

print(response.getcode())      # çŠ¶æ€ç 
print(response.geturl())       # URL
print(response.getheaders())   # å“åº”å¤´
```

---

### 3.3 ä¸‹è½½æ–‡ä»¶

```python
import urllib.request

# ä¸‹è½½ç½‘é¡µ
urllib.request.urlretrieve("http://www.baidu.com", "baidu.html")

# ä¸‹è½½å›¾ç‰‡
img_url = "https://www.python.org/static/img/python-logo.png"
urllib.request.urlretrieve(img_url, "python-logo.png")
```

#### ä¸‹è½½è¿›åº¦æ˜¾ç¤º

```python
from urllib.request import urlretrieve

def report_hook(blocknum, blocksize, totalsize):
    percent = min(blocknum * blocksize / totalsize, 1.0) * 100
    print(f"ä¸‹è½½è¿›åº¦: {percent:.2f}%")

urlretrieve("https://www.baidu.com/robots.txt", "robots.txt", reporthook=report_hook)
```

---

## å››ã€urllib.parse è§£æä¸æ„å»º URL

```python
from urllib.parse import urlparse

url = "https://www.example.com:8080/path/page?query=python#frag"
parsed = urlparse(url)

print(parsed.scheme)
print(parsed.hostname)
print(parsed.port)
print(parsed.path)
print(parsed.query)
print(parsed.fragment)
```

### URL ç¼–ç ä¸æ„å»º

```python
from urllib.parse import quote, urlencode

text = "Python ç¼–ç¨‹ & urllib"
print(quote(text))  # ç¼–ç 

params = {'wd': 'å‘¨æ°ä¼¦', 'sex': 'ç”·'}
print(urlencode(params))
```

---

## äº”ã€è®¾ç½® User-Agent æ¨¡æ‹Ÿæµè§ˆå™¨

```python
from urllib.request import Request, urlopen

url = "http://httpbin.org/user-agent"
headers = {'User-Agent': 'Mozilla/5.0 ...'}

req = Request(url, headers=headers)
res = urlopen(req)
print(res.read().decode('utf-8'))
```

---

## å…­ã€å‘é€ POST è¯·æ±‚

```python
import urllib.request
import urllib.parse
import json

url = "https://fanyi.baidu.com/sug"
data = {'kw': 'love'}
data = urllib.parse.urlencode(data).encode('utf-8')

headers = {'User-Agent': 'Mozilla/5.0 ...'}

req = urllib.request.Request(url=url, data=data, headers=headers)
response = urllib.request.urlopen(req)

result = response.read().decode('utf-8')
print(json.loads(result))
```

---

## ä¸ƒã€ä»£ç†è®¾ç½®

```python
import urllib.request
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

proxies = {
    "http": "http://user:pwd@proxy:port",
    "https": "http://user:pwd@proxy:port"
}

proxy_support = urllib.request.ProxyHandler(proxies)
opener = urllib.request.build_opener(proxy_support)

url = "http://httpbin.org/ip"
res = opener.open(url)
print(res.read().decode())
```

---

## å…«ã€Cookie ç™»å½•

```python
import urllib.request

headers = {
    'User-Agent': 'Mozilla/5.0 ...',
    'Cookie': 'your_cookie_here'
}

req = urllib.request.Request("https://www.example.com/personal", headers=headers)
response = urllib.request.urlopen(req)

with open("personal.html", "wb") as f:
    f.write(response.read())
```

---

## ä¹ã€ç»¼åˆç¤ºä¾‹ï¼šçˆ¬å–è±†ç“£ç”µå½± Top250

```python
import urllib.request
import os

headers = {'User-Agent': 'Mozilla/5.0 ...'}

for page in range(0, 250, 25):
    url = f"https://movie.douban.com/top250?start={page}&filter="
    req = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(req)
    content = response.read().decode('utf-8')

    filename = f"top250_page_{page//25+1}.html"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
```

---

## ğŸ”” æ³¨æ„äº‹é¡¹

1. **éµå®ˆæ³•å¾‹æ³•è§„**ï¼Œä¸è¦çˆ¬å–éšç§ã€æ•æ„Ÿä¿¡æ¯ã€‚
2. **å°Šé‡ç½‘ç«™æœåŠ¡**ï¼Œåˆç†æ§åˆ¶è®¿é—®é¢‘ç‡ï¼Œé¿å…å½±å“æœåŠ¡å™¨ã€‚
3. **åŠ å¼ºå¼‚å¸¸å¤„ç†**ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒã€‚
